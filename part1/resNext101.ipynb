{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen,urlretrieve\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.datasets import load_files   \n",
    "#from tensorflow.keras.utils import np_utils\n",
    "from glob import glob\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from classification_models.tfkeras import Classifiers\n",
    "\n",
    "class_names = sorted(os.listdir(r\"/home/nvme/data/train/train\"))\n",
    "\n",
    "N_classes = len(class_names)\n",
    "\n",
    "ResNext101, preprocess_input = Classifiers.get('resnext101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNext101(input_shape=(224,224,3), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "#x = Dense(4096, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(N_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "adam = Adam(lr = 0.0001)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import split_utils\n",
    "original_dir = \"/home/nvme/data/train/train\"\n",
    "validation_split = 0.2\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
    "# don't clear base_dir, because this directory holds on temp directory.\n",
    "base_dir, train_dir, val_dir = split_utils.train_valid_split(original_dir, validation_split, seed=1)\n",
    "\n",
    "# generator for train data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    class_mode = \"categorical\",\n",
    "    target_size = (224, 224),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "# generator for validation data\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    class_mode = \"categorical\",\n",
    "    target_size = (224, 224),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    seed = 42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "class_weights = {0: 65,\n",
    "                 1: 42,\n",
    "                 2: 5,\n",
    "                 3: 1,\n",
    "                 4: 4,\n",
    "                 5: 1,\n",
    "                 6: 169,\n",
    "                 7: 27,\n",
    "                 8: 13,\n",
    "                 9: 115,\n",
    "                 10: 2,\n",
    "                 11: 56,\n",
    "                 12: 70,\n",
    "                 13: 42,\n",
    "                 14: 11,\n",
    "                 15: 4,\n",
    "                 16: 7}\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "    steps_per_epoch = train_gen.samples // batch_size,\n",
    "    validation_data = val_gen, \n",
    "    validation_steps = val_gen.samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:2491]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[2491:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.fit_generator(train_gen,\n",
    "    steps_per_epoch = train_gen.samples // batch_size,\n",
    "    validation_data = val_gen, \n",
    "    validation_steps = val_gen.samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    class_weight = class_weights\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_test = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "test_gen = datagen_test.flow_from_directory('/home/nvme/data/test',\n",
    "                                        target_size = (224, 224),\n",
    "                                        batch_size = 1,\n",
    "                                        shuffle = False)\n",
    "\n",
    "pred = model.predict_generator(test_gen, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.argmax(pred, axis = 1)\n",
    "predictions = [class_names[k] for k in p]\n",
    "a = np.arange(len(predictions))\n",
    "d = {'Id': a, 'Category': predictions}\n",
    "\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "df.to_csv('submission.csv', index = None, header = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

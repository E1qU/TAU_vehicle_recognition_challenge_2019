{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caterpillar', 'Car', 'Truck', 'Segway', 'Limousine', 'Motorcycle', 'Tank', 'Barge', 'Helicopter', 'Bicycle', 'Bus', 'Snowmobile', 'Boat', 'Cart', 'Taxi', 'Van', 'Ambulance']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "base_dir = '/home/nvme/data/openimg'\n",
    "\n",
    "# Create training folder\n",
    "files = os.listdir(base_dir + '/train')\n",
    "print(files)\n",
    "\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pathes and class encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/nvme/data/openimg/train/Bus/000343_12.jpg', 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torchvision.datasets.ImageFolder(base_dir + '/train')\n",
    "pathes = dataset.samples[:len(dataset)]\n",
    "for i in range(len(pathes)):\n",
    "    pathes[i] = pathes[i][0]\n",
    "labels = dataset.targets\n",
    "pathes[10984], labels[10984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ambulance': 132,\n",
       " 'Barge': 202,\n",
       " 'Bicycle': 1618,\n",
       " 'Boat': 8695,\n",
       " 'Bus': 2133,\n",
       " 'Car': 6782,\n",
       " 'Cart': 51,\n",
       " 'Caterpillar': 331,\n",
       " 'Helicopter': 668,\n",
       " 'Limousine': 74,\n",
       " 'Motorcycle': 2986,\n",
       " 'Segway': 153,\n",
       " 'Snowmobile': 123,\n",
       " 'Tank': 206,\n",
       " 'Taxi': 748,\n",
       " 'Truck': 2033,\n",
       " 'Van': 1111}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = base_dir + '/train/'\n",
    "data = []\n",
    "for category in sorted(os.listdir(path)):\n",
    "    for file in sorted(os.listdir(os.path.join(path, category))):\n",
    "        data.append((category, os.path.join(path, category,  file)))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['class', 'file_path'])\n",
    "counts = df['class'].value_counts().to_dict()\n",
    "df_counts = dataset.class_to_idx.copy()\n",
    "for key in df_counts:\n",
    "    df_counts[key] = counts[key]\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, list_IDs, pathes, labels, phase, transforms=None):\n",
    "        'Initialization'\n",
    "        self.pathes = pathes\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.transforms = transforms\n",
    "        self.phase = phase\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        \n",
    "        # Load data and get label\n",
    "        \n",
    "        img = Image.open(self.pathes[ID])\n",
    "        if self.transforms is not None:\n",
    "            X = self.transforms(img)\n",
    "            \n",
    "        if self.phase in ['train', 'val']:\n",
    "            y = self.labels[ID]\n",
    "            return X, y\n",
    "        else:\n",
    "            return X, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / val random split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(indices, start, end):\n",
    "    return indices[start : start + end]\n",
    "\n",
    "\n",
    "TRAIN_PCT, VALIDATION_PCT = 0.75, 0.25  # rest will go for test\n",
    "train_count = int(len(dataset) * TRAIN_PCT)\n",
    "validation_count = int(len(dataset) * TRAIN_PCT)\n",
    "\n",
    "indices = torch.randperm(len(dataset))\n",
    "\n",
    "train_indices = get_subset(indices, 0, train_count)\n",
    "validation_indices = get_subset(indices, train_count, validation_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices (list, optional): a list of indices\n",
    "        num_samples (int, optional): number of samples to draw\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices=None, num_samples=None):\n",
    "                \n",
    "        # if indices is not provided, \n",
    "        # all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) \\\n",
    "            if indices is None else indices\n",
    "            \n",
    "        # if num_samples is not provided, \n",
    "        # draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) \\\n",
    "            if num_samples is None else num_samples\n",
    "            \n",
    "        # distribution of classes in the dataset \n",
    "        label_to_count = {}\n",
    "        for idx in self.indices:\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label in label_to_count:\n",
    "                label_to_count[label] += 1\n",
    "            else:\n",
    "                label_to_count[label] = 1\n",
    "                \n",
    "        # weight for each sample\n",
    "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
    "                   for idx in self.indices]\n",
    "        self.weights = torch.DoubleTensor(weights)\n",
    "\n",
    "    def _get_label(self, dataset, idx):\n",
    "        i = dataset.list_IDs[idx]\n",
    "        return dataset.labels[i]\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(\n",
    "            self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([           \n",
    "    transforms.Resize(256),                    \n",
    "    transforms.CenterCrop(224),                \n",
    "    transforms.ToTensor(),                     \n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],                \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([          \n",
    "    transforms.Resize(256),                    \n",
    "    transforms.CenterCrop(224),               \n",
    "    transforms.ToTensor(),                     \n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],               \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True # not working\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 100\n",
    "\n",
    "# Datasets\n",
    "partition = {'train': train_indices.tolist(), 'validation': validation_indices.tolist()}\n",
    "labels = {}\n",
    "for i in indices:\n",
    "    labels[int(i)] = dataset.samples[indices[i]][1]\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], pathes, labels, 'train', train_transforms)\n",
    "training_generator = data.DataLoader(training_set, **params\n",
    "                                     , sampler = ImbalancedDatasetSampler(training_set, num_samples = 17000)\n",
    "                                    )\n",
    "\n",
    "validation_set = Dataset(partition['validation'], pathes, labels, 'val', val_transforms)\n",
    "validation_generator = data.DataLoader(validation_set, **params\n",
    "                                      , sampler = ImbalancedDatasetSampler(validation_set, num_samples = 5661)\n",
    "                                      )\n",
    "\n",
    "dataloaders = {'train': training_generator, 'val' : validation_generator}\n",
    "dataset_sizes = {'train': len(training_set.list_IDs), 'val': len(validation_set.list_IDs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esa/miniconda3/envs/openimg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/esa/miniconda3/envs/openimg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/esa/miniconda3/envs/openimg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/esa/miniconda3/envs/openimg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/esa/miniconda3/envs/openimg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/esa/miniconda3/envs/openimg/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "avgpool\n",
      "classifier\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "#resnet = models.resnet(pretrained=True)\n",
    "#num_ftrs = resnet.fc.in_features\n",
    "#resnet.fc = nn.Linear(num_ftrs, 17)\n",
    "\n",
    "vgg = models.vgg11_bn(pretrained=False)\n",
    "#set_parameter_requires_grad(vgg, feature_extract)\n",
    "num_ftrs = vgg.classifier[6].in_features\n",
    "vgg.classifier[6] = nn.Linear(num_ftrs,17)\n",
    "input_size = 224\n",
    "\n",
    "for name, child in vgg.named_children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = vgg.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(vgg.parameters(), lr=3e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    iter_num_train = 0\n",
    "    iter_num_val = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            current_loss = 0.0\n",
    "            current_corrects = 0\n",
    "\n",
    "            # Here's where the training happens\n",
    "            print('Iterating through data...')\n",
    "\n",
    "            for inputs, labels in tqdm(dataloaders[phase], desc=f'{phase} epoch ({epoch})'):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # We need to zero the gradients, don't forget it\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Time to carry out the forward training poss\n",
    "                # We only need to log the loss stats if we are in training phase\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # We want variables to hold the loss statistics\n",
    "                current_loss += loss.item() * inputs.size(0)\n",
    "                current_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                #writer.add_scalar(f'Loss/{phase}', loss.item()/params['batch_size'], iter_num)\n",
    "                #writer.add_scalar(f'Accuracy/{phase}', torch.sum(preds == labels.data)/params['batch_size'], iter_num)\n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    writer.add_scalar(f'Loss/{phase}', loss.item()/params['batch_size'], iter_num_train)\n",
    "                    writer.add_scalar(f'Accuracy/{phase}', torch.sum(preds == labels.data)/params['batch_size'], iter_num_train)\n",
    "                    iter_num_train += 1\n",
    "                else:\n",
    "                    writer.add_scalar(f'Loss/{phase}', loss.item()/params['batch_size'], iter_num_val)\n",
    "                    writer.add_scalar(f'Accuracy/{phase}', torch.sum(preds == labels.data)/params['batch_size'], iter_num_val)\n",
    "                    iter_num_val += 1                    \n",
    "                \n",
    "\n",
    "            epoch_loss = current_loss / dataset_sizes[phase]\n",
    "            epoch_acc = current_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Make a copy of the model if the accuracy on the validation set has improved\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_since = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_since // 60, time_since % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # Now we'll load in the best model weights and return it\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/esa/miniconda3/envs/openimg/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "train epoch (0):   0%|          | 0/266 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "Iterating through data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch (0): 100%|██████████| 266/266 [01:30<00:00,  2.93it/s]\n",
      "val epoch (0):   0%|          | 0/89 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.4633 Acc: 0.0537\n",
      "Iterating through data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val epoch (0): 100%|██████████| 89/89 [00:26<00:00,  3.40it/s]\n",
      "train epoch (1):   0%|          | 0/266 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.2914 Acc: 0.0411\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "Iterating through data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch (1):  77%|███████▋  | 206/266 [01:10<00:20,  2.98it/s]"
     ]
    }
   ],
   "source": [
    "base_model = train_model(vgg, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 21036, 'val': 7012}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes = {'train': len(training_set.list_IDs), 'val': len(validation_set.list_IDs)}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_val = []\n",
    "for i, (x, y) in enumerate(validation_generator):\n",
    "    l_val.append(y.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_train = []\n",
    "for x, y in training_generator:\n",
    "    l_train.append(y.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_val), len(l_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.bincount(l_val)\n",
    "ii_val = np.nonzero(y_val)[0]\n",
    "\n",
    "y_train = np.bincount(l_train)\n",
    "ii_train = np.nonzero(y_train)[0]\n",
    "\n",
    "counts = dataset.class_to_idx.copy()\n",
    "i = 0\n",
    "for key in counts:\n",
    "    counts[key] = [y_train[i], y_val[i], df_counts[key]]\n",
    "    i += 1\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target: 224 x 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = {'train' : training_generator, 'val' : validation_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators['train']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
